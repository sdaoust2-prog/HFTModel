{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Stock Prediction\n",
    "\n",
    "Following the quantitative approach:\n",
    "- Phase 1: Single best feature model (z-scored)\n",
    "- Phase 2: Multi-feature linear regression\n",
    "- Phase 3: Regularized models (Ridge, Lasso)\n",
    "\n",
    "Compare all models on out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import joblib\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "API_KEY = \"vFDjkUVRfPnedLrbRjm75BZ9CJHz3dfv\"\n",
    "TICKER = \"AAPL\"\n",
    "START_DATE = \"2025-10-01\"\n",
    "END_DATE = \"2025-11-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_polygon_data(ticker, start, end, api_key):\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{start}/{end}?apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if 'results' not in data or len(data['results']) < 2:\n",
    "        raise ValueError(\"not enough data\")\n",
    "    \n",
    "    df = pd.DataFrame(data['results'])\n",
    "    df['timestamp'] = pd.to_datetime(df['t'], unit='ms')\n",
    "    df = df.rename(columns={'o':'open','h':'high','l':'low','c':'close','v':'volume'})\n",
    "    df = df[['timestamp','open','high','low','close','volume']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['momentum_1min'] = df['close'].pct_change()\n",
    "    df['volatility_1min'] = df['momentum_1min'] ** 2\n",
    "    df['price_direction'] = (df['close'] > df['open']).astype(int)\n",
    "    df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()\n",
    "    df['vwap_dev'] = (df['close'] - df['vwap']) / df['vwap']\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['minute'] = df['timestamp'].dt.minute\n",
    "    \n",
    "    # target: next minute return (continuous, not binary)\n",
    "    df['next_return'] = df['close'].shift(-1) / df['close'] - 1\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "print(f\"loading data for {TICKER}...\")\n",
    "df = pull_polygon_data(TICKER, START_DATE, END_DATE, API_KEY)\n",
    "df = calculate_features(df)\n",
    "print(f\"loaded {len(df)} bars\")\n",
    "\n",
    "features = ['momentum_1min', 'volatility_1min', 'price_direction', 'vwap_dev', 'hour', 'minute']\n",
    "X = df[features]\n",
    "y = df['next_return']\n",
    "\n",
    "# chronological split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"\\ntrain size: {len(X_train)}\")\n",
    "print(f\"test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Single Best Feature Model\n",
    "\n",
    "Use only the feature with highest IC, z-scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# find best feature by IC on training data\n",
    "ic_scores = {}\n",
    "for feat in features:\n",
    "    ic, _ = pearsonr(X_train[feat], y_train)\n",
    "    ic_scores[feat] = abs(ic)\n",
    "\n",
    "best_feature = max(ic_scores, key=ic_scores.get)\n",
    "print(f\"best feature: {best_feature} (IC={ic_scores[best_feature]:.4f})\")\n",
    "\n",
    "# z-score the feature\n",
    "scaler = StandardScaler()\n",
    "X_train_single = scaler.fit_transform(X_train[[best_feature]])\n",
    "X_test_single = scaler.transform(X_test[[best_feature]])\n",
    "\n",
    "# fit simple linear regression\n",
    "model_single = LinearRegression()\n",
    "model_single.fit(X_train_single, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_train_single = model_single.predict(X_train_single)\n",
    "y_pred_test_single = model_single.predict(X_test_single)\n",
    "\n",
    "# metrics\n",
    "train_corr_single, _ = pearsonr(y_train, y_pred_train_single)\n",
    "test_corr_single, _ = pearsonr(y_test, y_pred_test_single)\n",
    "train_mse_single = mean_squared_error(y_train, y_pred_train_single)\n",
    "test_mse_single = mean_squared_error(y_test, y_pred_test_single)\n",
    "\n",
    "print(f\"\\nPhase 1 Results (Single Feature: {best_feature}):\")\n",
    "print(f\"train correlation: {train_corr_single:.4f}\")\n",
    "print(f\"test correlation: {test_corr_single:.4f}\")\n",
    "print(f\"train MSE: {train_mse_single:.8f}\")\n",
    "print(f\"test MSE: {test_mse_single:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Multi-Feature Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize all features\n",
    "scaler_multi = StandardScaler()\n",
    "X_train_scaled = scaler_multi.fit_transform(X_train)\n",
    "X_test_scaled = scaler_multi.transform(X_test)\n",
    "\n",
    "# fit linear regression\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_train_linear = model_linear.predict(X_train_scaled)\n",
    "y_pred_test_linear = model_linear.predict(X_test_scaled)\n",
    "\n",
    "# metrics\n",
    "train_corr_linear, _ = pearsonr(y_train, y_pred_train_linear)\n",
    "test_corr_linear, _ = pearsonr(y_test, y_pred_test_linear)\n",
    "train_mse_linear = mean_squared_error(y_train, y_pred_train_linear)\n",
    "test_mse_linear = mean_squared_error(y_test, y_pred_test_linear)\n",
    "\n",
    "print(f\"\\nPhase 2 Results (Multi-Feature Linear):\")\n",
    "print(f\"train correlation: {train_corr_linear:.4f}\")\n",
    "print(f\"test correlation: {test_corr_linear:.4f}\")\n",
    "print(f\"train MSE: {train_mse_linear:.8f}\")\n",
    "print(f\"test MSE: {test_mse_linear:.8f}\")\n",
    "\n",
    "# feature coefficients\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for feat, coef in zip(features, model_linear.coef_):\n",
    "    print(f\"  {feat}: {coef:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "ridge_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model_ridge = Ridge(alpha=alpha)\n",
    "    model_ridge.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred_train_ridge = model_ridge.predict(X_train_scaled)\n",
    "    y_pred_test_ridge = model_ridge.predict(X_test_scaled)\n",
    "    \n",
    "    train_corr, _ = pearsonr(y_train, y_pred_train_ridge)\n",
    "    test_corr, _ = pearsonr(y_test, y_pred_test_ridge)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test_ridge)\n",
    "    \n",
    "    ridge_results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_corr': train_corr,\n",
    "        'test_corr': test_corr,\n",
    "        'test_mse': test_mse\n",
    "    })\n",
    "\n",
    "ridge_df = pd.DataFrame(ridge_results)\n",
    "print(\"\\nRidge Regression Results:\")\n",
    "print(ridge_df.to_string(index=False))\n",
    "\n",
    "# best ridge model\n",
    "best_ridge_alpha = ridge_df.loc[ridge_df['test_corr'].idxmax(), 'alpha']\n",
    "print(f\"\\nbest ridge alpha: {best_ridge_alpha}\")\n",
    "\n",
    "model_ridge_best = Ridge(alpha=best_ridge_alpha)\n",
    "model_ridge_best.fit(X_train_scaled, y_train)\n",
    "y_pred_test_ridge_best = model_ridge_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different alpha values\n",
    "lasso_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model_lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    model_lasso.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred_train_lasso = model_lasso.predict(X_train_scaled)\n",
    "    y_pred_test_lasso = model_lasso.predict(X_test_scaled)\n",
    "    \n",
    "    train_corr, _ = pearsonr(y_train, y_pred_train_lasso)\n",
    "    test_corr, _ = pearsonr(y_test, y_pred_test_lasso)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test_lasso)\n",
    "    \n",
    "    # count non-zero coefficients\n",
    "    non_zero = np.sum(model_lasso.coef_ != 0)\n",
    "    \n",
    "    lasso_results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_corr': train_corr,\n",
    "        'test_corr': test_corr,\n",
    "        'test_mse': test_mse,\n",
    "        'non_zero_features': non_zero\n",
    "    })\n",
    "\n",
    "lasso_df = pd.DataFrame(lasso_results)\n",
    "print(\"\\nLasso Regression Results:\")\n",
    "print(lasso_df.to_string(index=False))\n",
    "\n",
    "# best lasso model\n",
    "best_lasso_alpha = lasso_df.loc[lasso_df['test_corr'].idxmax(), 'alpha']\n",
    "print(f\"\\nbest lasso alpha: {best_lasso_alpha}\")\n",
    "\n",
    "model_lasso_best = Lasso(alpha=best_lasso_alpha, max_iter=10000)\n",
    "model_lasso_best.fit(X_train_scaled, y_train)\n",
    "y_pred_test_lasso_best = model_lasso_best.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nLasso Feature Selection:\")\n",
    "for feat, coef in zip(features, model_lasso_best.coef_):\n",
    "    print(f\"  {feat}: {coef:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all models\n",
    "test_corr_ridge_best, _ = pearsonr(y_test, y_pred_test_ridge_best)\n",
    "test_corr_lasso_best, _ = pearsonr(y_test, y_pred_test_lasso_best)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'model': 'Single Feature',\n",
    "        'train_corr': train_corr_single,\n",
    "        'test_corr': test_corr_single,\n",
    "        'test_mse': test_mse_single,\n",
    "        'overfitting_gap': train_corr_single - test_corr_single\n",
    "    },\n",
    "    {\n",
    "        'model': 'Linear (All Features)',\n",
    "        'train_corr': train_corr_linear,\n",
    "        'test_corr': test_corr_linear,\n",
    "        'test_mse': test_mse_linear,\n",
    "        'overfitting_gap': train_corr_linear - test_corr_linear\n",
    "    },\n",
    "    {\n",
    "        'model': f'Ridge (α={best_ridge_alpha})',\n",
    "        'train_corr': ridge_df.loc[ridge_df['alpha'] == best_ridge_alpha, 'train_corr'].values[0],\n",
    "        'test_corr': test_corr_ridge_best,\n",
    "        'test_mse': ridge_df.loc[ridge_df['alpha'] == best_ridge_alpha, 'test_mse'].values[0],\n",
    "        'overfitting_gap': ridge_df.loc[ridge_df['alpha'] == best_ridge_alpha, 'train_corr'].values[0] - test_corr_ridge_best\n",
    "    },\n",
    "    {\n",
    "        'model': f'Lasso (α={best_lasso_alpha})',\n",
    "        'train_corr': lasso_df.loc[lasso_df['alpha'] == best_lasso_alpha, 'train_corr'].values[0],\n",
    "        'test_corr': test_corr_lasso_best,\n",
    "        'test_mse': lasso_df.loc[lasso_df['alpha'] == best_lasso_alpha, 'test_mse'].values[0],\n",
    "        'overfitting_gap': lasso_df.loc[lasso_df['alpha'] == best_lasso_alpha, 'train_corr'].values[0] - test_corr_lasso_best\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction vs Actual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models_to_plot = [\n",
    "    ('Single Feature', y_pred_test_single, test_corr_single),\n",
    "    ('Linear (All)', y_pred_test_linear, test_corr_linear),\n",
    "    ('Ridge', y_pred_test_ridge_best, test_corr_ridge_best),\n",
    "    ('Lasso', y_pred_test_lasso_best, test_corr_lasso_best)\n",
    "]\n",
    "\n",
    "for idx, (name, y_pred, corr) in enumerate(models_to_plot):\n",
    "    row, col = idx // 2, idx % 2\n",
    "    axes[row, col].scatter(y_test, y_pred, alpha=0.3, s=10)\n",
    "    axes[row, col].set_xlabel('actual return')\n",
    "    axes[row, col].set_ylabel('predicted return')\n",
    "    axes[row, col].set_title(f'{name} (corr={corr:.4f})')\n",
    "    axes[row, col].axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "    axes[row, col].axvline(x=0, color='red', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # add diagonal line (perfect prediction)\n",
    "    lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "    axes[row, col].plot(lims, lims, 'k--', alpha=0.5, label='perfect prediction')\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine best model by test correlation\n",
    "best_idx = comparison['test_corr'].idxmax()\n",
    "best_model_name = comparison.loc[best_idx, 'model']\n",
    "\n",
    "print(f\"\\nbest model: {best_model_name}\")\n",
    "print(f\"test correlation: {comparison.loc[best_idx, 'test_corr']:.4f}\")\n",
    "\n",
    "# save the best linear model\n",
    "if 'Ridge' in best_model_name:\n",
    "    joblib.dump(model_ridge_best, 'trained_linear_model.pkl')\n",
    "    joblib.dump(scaler_multi, 'feature_scaler.pkl')\n",
    "elif 'Lasso' in best_model_name:\n",
    "    joblib.dump(model_lasso_best, 'trained_linear_model.pkl')\n",
    "    joblib.dump(scaler_multi, 'feature_scaler.pkl')\n",
    "elif 'Linear' in best_model_name:\n",
    "    joblib.dump(model_linear, 'trained_linear_model.pkl')\n",
    "    joblib.dump(scaler_multi, 'feature_scaler.pkl')\n",
    "else:\n",
    "    joblib.dump(model_single, 'trained_linear_model.pkl')\n",
    "    joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "\n",
    "print(f\"\\nsaved best model to trained_linear_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
