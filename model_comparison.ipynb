{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: All Approaches\n",
    "\n",
    "Train and compare all models on the same data:\n",
    "1. Single best feature (z-scored)\n",
    "2. Linear regression (all features)\n",
    "3. Ridge regression (L2 regularization)\n",
    "4. Lasso regression (L1 regularization)\n",
    "5. Random Forest classifier\n",
    "\n",
    "Compare on:\n",
    "- Out-of-sample accuracy/correlation\n",
    "- Overfitting gap (train - test)\n",
    "- Sharpe ratio from backtesting\n",
    "- Win rate\n",
    "- Profit factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import joblib\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"vFDjkUVRfPnedLrbRjm75BZ9CJHz3dfv\"\n",
    "TICKER = \"AAPL\"\n",
    "START_DATE = \"2025-10-01\"\n",
    "END_DATE = \"2025-11-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_polygon_data(ticker, start, end, api_key):\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{start}/{end}?apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if 'results' not in data or len(data['results']) < 2:\n",
    "        raise ValueError(\"not enough data\")\n",
    "    \n",
    "    df = pd.DataFrame(data['results'])\n",
    "    df['timestamp'] = pd.to_datetime(df['t'], unit='ms')\n",
    "    df = df.rename(columns={'o':'open','h':'high','l':'low','c':'close','v':'volume'})\n",
    "    df = df[['timestamp','open','high','low','close','volume']]\n",
    "    return df\n",
    "\n",
    "def calculate_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['momentum_1min'] = df['close'].pct_change()\n",
    "    df['volatility_1min'] = df['momentum_1min'] ** 2\n",
    "    df['price_direction'] = (df['close'] > df['open']).astype(int)\n",
    "    df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()\n",
    "    df['vwap_dev'] = (df['close'] - df['vwap']) / df['vwap']\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['minute'] = df['timestamp'].dt.minute\n",
    "    \n",
    "    df['next_return'] = df['close'].shift(-1) / df['close'] - 1\n",
    "    df['target_binary'] = (df['next_return'] > 0).astype(int)\n",
    "    df['target_continuous'] = df['next_return']\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pull_polygon_data(TICKER, START_DATE, END_DATE, API_KEY)\n",
    "df = calculate_features(df)\n",
    "\n",
    "features = ['momentum_1min', 'volatility_1min', 'price_direction', 'vwap_dev', 'hour', 'minute']\n",
    "X = df[features]\n",
    "y_binary = df['target_binary']\n",
    "y_continuous = df['target_continuous']\n",
    "\n",
    "# chronological split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train_bin, y_test_bin = y_binary.iloc[:split_idx], y_binary.iloc[split_idx:]\n",
    "y_train_cont, y_test_cont = y_continuous.iloc[:split_idx], y_continuous.iloc[split_idx:]\n",
    "\n",
    "print(f\"loaded {len(df)} bars\")\n",
    "print(f\"train: {len(X_train)}, test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Best Feature Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best feature by IC\n",
    "ic_scores = {feat: abs(pearsonr(X_train[feat], y_train_cont)[0]) for feat in features}\n",
    "best_feature = max(ic_scores, key=ic_scores.get)\n",
    "print(f\"best feature: {best_feature}\")\n",
    "\n",
    "# z-score and train\n",
    "scaler_single = StandardScaler()\n",
    "X_train_single = scaler_single.fit_transform(X_train[[best_feature]])\n",
    "X_test_single = scaler_single.transform(X_test[[best_feature]])\n",
    "\n",
    "model_single = LinearRegression()\n",
    "model_single.fit(X_train_single, y_train_cont)\n",
    "\n",
    "pred_train_single = model_single.predict(X_train_single)\n",
    "pred_test_single = model_single.predict(X_test_single)\n",
    "\n",
    "# convert to binary for accuracy\n",
    "pred_train_single_bin = (pred_train_single > 0).astype(int)\n",
    "pred_test_single_bin = (pred_test_single > 0).astype(int)\n",
    "\n",
    "single_train_corr = pearsonr(y_train_cont, pred_train_single)[0]\n",
    "single_test_corr = pearsonr(y_test_cont, pred_test_single)[0]\n",
    "single_train_acc = accuracy_score(y_train_bin, pred_train_single_bin)\n",
    "single_test_acc = accuracy_score(y_test_bin, pred_test_single_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Feature Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_multi = StandardScaler()\n",
    "X_train_scaled = scaler_multi.fit_transform(X_train)\n",
    "X_test_scaled = scaler_multi.transform(X_test)\n",
    "\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X_train_scaled, y_train_cont)\n",
    "\n",
    "pred_train_linear = model_linear.predict(X_train_scaled)\n",
    "pred_test_linear = model_linear.predict(X_test_scaled)\n",
    "\n",
    "pred_train_linear_bin = (pred_train_linear > 0).astype(int)\n",
    "pred_test_linear_bin = (pred_test_linear > 0).astype(int)\n",
    "\n",
    "linear_train_corr = pearsonr(y_train_cont, pred_train_linear)[0]\n",
    "linear_test_corr = pearsonr(y_test_cont, pred_test_linear)[0]\n",
    "linear_train_acc = accuracy_score(y_train_bin, pred_train_linear_bin)\n",
    "linear_test_acc = accuracy_score(y_test_bin, pred_test_linear_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Ridge(alpha=0.1)\n",
    "model_ridge.fit(X_train_scaled, y_train_cont)\n",
    "\n",
    "pred_train_ridge = model_ridge.predict(X_train_scaled)\n",
    "pred_test_ridge = model_ridge.predict(X_test_scaled)\n",
    "\n",
    "pred_train_ridge_bin = (pred_train_ridge > 0).astype(int)\n",
    "pred_test_ridge_bin = (pred_test_ridge > 0).astype(int)\n",
    "\n",
    "ridge_train_corr = pearsonr(y_train_cont, pred_train_ridge)[0]\n",
    "ridge_test_corr = pearsonr(y_test_cont, pred_test_ridge)[0]\n",
    "ridge_train_acc = accuracy_score(y_train_bin, pred_train_ridge_bin)\n",
    "ridge_test_acc = accuracy_score(y_test_bin, pred_test_ridge_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha=0.0001, max_iter=10000)\n",
    "model_lasso.fit(X_train_scaled, y_train_cont)\n",
    "\n",
    "pred_train_lasso = model_lasso.predict(X_train_scaled)\n",
    "pred_test_lasso = model_lasso.predict(X_test_scaled)\n",
    "\n",
    "pred_train_lasso_bin = (pred_train_lasso > 0).astype(int)\n",
    "pred_test_lasso_bin = (pred_test_lasso > 0).astype(int)\n",
    "\n",
    "lasso_train_corr = pearsonr(y_train_cont, pred_train_lasso)[0]\n",
    "lasso_test_corr = pearsonr(y_test_cont, pred_test_lasso)[0]\n",
    "lasso_train_acc = accuracy_score(y_train_bin, pred_train_lasso_bin)\n",
    "lasso_test_acc = accuracy_score(y_test_bin, pred_test_lasso_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train, y_train_bin)\n",
    "\n",
    "pred_train_rf = model_rf.predict(X_train)\n",
    "pred_test_rf = model_rf.predict(X_test)\n",
    "\n",
    "prob_train_rf = model_rf.predict_proba(X_train)[:, 1]\n",
    "prob_test_rf = model_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "rf_train_acc = accuracy_score(y_train_bin, pred_train_rf)\n",
    "rf_test_acc = accuracy_score(y_test_bin, pred_test_rf)\n",
    "rf_train_auc = roc_auc_score(y_train_bin, prob_train_rf)\n",
    "rf_test_auc = roc_auc_score(y_test_bin, prob_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'model': f'Single Feature ({best_feature})',\n",
    "        'type': 'regression',\n",
    "        'train_corr': single_train_corr,\n",
    "        'test_corr': single_test_corr,\n",
    "        'train_acc': single_train_acc,\n",
    "        'test_acc': single_test_acc,\n",
    "        'corr_gap': single_train_corr - single_test_corr,\n",
    "        'acc_gap': single_train_acc - single_test_acc\n",
    "    },\n",
    "    {\n",
    "        'model': 'Linear Regression',\n",
    "        'type': 'regression',\n",
    "        'train_corr': linear_train_corr,\n",
    "        'test_corr': linear_test_corr,\n",
    "        'train_acc': linear_train_acc,\n",
    "        'test_acc': linear_test_acc,\n",
    "        'corr_gap': linear_train_corr - linear_test_corr,\n",
    "        'acc_gap': linear_train_acc - linear_test_acc\n",
    "    },\n",
    "    {\n",
    "        'model': 'Ridge (α=0.1)',\n",
    "        'type': 'regression',\n",
    "        'train_corr': ridge_train_corr,\n",
    "        'test_corr': ridge_test_corr,\n",
    "        'train_acc': ridge_train_acc,\n",
    "        'test_acc': ridge_test_acc,\n",
    "        'corr_gap': ridge_train_corr - ridge_test_corr,\n",
    "        'acc_gap': ridge_train_acc - ridge_test_acc\n",
    "    },\n",
    "    {\n",
    "        'model': 'Lasso (α=0.0001)',\n",
    "        'type': 'regression',\n",
    "        'train_corr': lasso_train_corr,\n",
    "        'test_corr': lasso_test_corr,\n",
    "        'train_acc': lasso_train_acc,\n",
    "        'test_acc': lasso_test_acc,\n",
    "        'corr_gap': lasso_train_corr - lasso_test_corr,\n",
    "        'acc_gap': lasso_train_acc - lasso_test_acc\n",
    "    },\n",
    "    {\n",
    "        'model': 'Random Forest',\n",
    "        'type': 'classifier',\n",
    "        'train_corr': rf_train_auc,\n",
    "        'test_corr': rf_test_auc,\n",
    "        'train_acc': rf_train_acc,\n",
    "        'test_acc': rf_test_acc,\n",
    "        'corr_gap': rf_train_auc - rf_test_auc,\n",
    "        'acc_gap': rf_train_acc - rf_test_acc\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(\"\\nNote: For regression models, 'corr' = Pearson correlation. For RF, 'corr' = AUC.\")\n",
    "print(comparison[['model', 'test_corr', 'test_acc', 'corr_gap', 'acc_gap']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# test accuracy comparison\n",
    "axes[0].barh(comparison['model'], comparison['test_acc'], color='steelblue')\n",
    "axes[0].set_xlabel('Test Accuracy')\n",
    "axes[0].set_title('Out-of-Sample Accuracy Comparison')\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# overfitting gap comparison\n",
    "colors = ['green' if gap < 0.05 else 'orange' if gap < 0.1 else 'red' for gap in comparison['acc_gap']]\n",
    "axes[1].barh(comparison['model'], comparison['acc_gap'], color=colors)\n",
    "axes[1].set_xlabel('Overfitting Gap (train_acc - test_acc)')\n",
    "axes[1].set_title('Overfitting Analysis')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_backtest(signals, returns, transaction_cost=0.0001):\n",
    "    \"\"\"Simple backtest given signals and actual returns\"\"\"\n",
    "    signals = np.array(signals)\n",
    "    returns = np.array(returns)\n",
    "    \n",
    "    # calculate strategy returns\n",
    "    position_changes = np.diff(signals, prepend=0)\n",
    "    strategy_returns = signals * returns - np.abs(position_changes) * transaction_cost\n",
    "    \n",
    "    # metrics\n",
    "    total_return = (1 + strategy_returns).prod() - 1\n",
    "    sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252 * 390) if strategy_returns.std() > 0 else 0\n",
    "    \n",
    "    trades = strategy_returns[strategy_returns != 0]\n",
    "    win_rate = (trades > 0).sum() / len(trades) if len(trades) > 0 else 0\n",
    "    \n",
    "    winning = trades[trades > 0]\n",
    "    losing = trades[trades < 0]\n",
    "    profit_factor = winning.sum() / abs(losing.sum()) if len(losing) > 0 and losing.sum() != 0 else np.inf\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'sharpe': sharpe,\n",
    "        'win_rate': win_rate,\n",
    "        'profit_factor': profit_factor,\n",
    "        'num_trades': len(trades)\n",
    "    }\n",
    "\n",
    "# convert predictions to signals (1 = long, -1 = short, 0 = flat)\n",
    "# for regression models, use threshold of 0\n",
    "signal_single = np.where(pred_test_single > 0, 1, -1)\n",
    "signal_linear = np.where(pred_test_linear > 0, 1, -1)\n",
    "signal_ridge = np.where(pred_test_ridge > 0, 1, -1)\n",
    "signal_lasso = np.where(pred_test_lasso > 0, 1, -1)\n",
    "\n",
    "# for RF, use probability threshold 0.55\n",
    "signal_rf = np.where(prob_test_rf > 0.55, 1, np.where(prob_test_rf < 0.45, -1, 0))\n",
    "\n",
    "# run backtests\n",
    "bt_single = simple_backtest(signal_single, y_test_cont)\n",
    "bt_linear = simple_backtest(signal_linear, y_test_cont)\n",
    "bt_ridge = simple_backtest(signal_ridge, y_test_cont)\n",
    "bt_lasso = simple_backtest(signal_lasso, y_test_cont)\n",
    "bt_rf = simple_backtest(signal_rf, y_test_cont)\n",
    "\n",
    "# buy and hold benchmark\n",
    "bt_bh = simple_backtest(np.ones(len(y_test_cont)), y_test_cont)\n",
    "\n",
    "backtest_comparison = pd.DataFrame([\n",
    "    {'model': f'Single ({best_feature})', **bt_single},\n",
    "    {'model': 'Linear', **bt_linear},\n",
    "    {'model': 'Ridge', **bt_ridge},\n",
    "    {'model': 'Lasso', **bt_lasso},\n",
    "    {'model': 'Random Forest', **bt_rf},\n",
    "    {'model': 'Buy & Hold', **bt_bh}\n",
    "])\n",
    "\n",
    "print(\"\\n=== BACKTEST COMPARISON ===\")\n",
    "print(backtest_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(\"\\nBest Test Accuracy:\")\n",
    "best_acc_idx = comparison['test_acc'].idxmax()\n",
    "print(f\"  {comparison.loc[best_acc_idx, 'model']}: {comparison.loc[best_acc_idx, 'test_acc']:.4f}\")\n",
    "\n",
    "print(\"\\nLowest Overfitting:\")\n",
    "best_gap_idx = comparison['acc_gap'].idxmin()\n",
    "print(f\"  {comparison.loc[best_gap_idx, 'model']}: gap = {comparison.loc[best_gap_idx, 'acc_gap']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Sharpe Ratio:\")\n",
    "best_sharpe_idx = backtest_comparison['sharpe'].idxmax()\n",
    "print(f\"  {backtest_comparison.loc[best_sharpe_idx, 'model']}: {backtest_comparison.loc[best_sharpe_idx, 'sharpe']:.2f}\")\n",
    "\n",
    "print(\"\\nBest Total Return:\")\n",
    "best_return_idx = backtest_comparison['total_return'].idxmax()\n",
    "print(f\"  {backtest_comparison.loc[best_return_idx, 'model']}: {backtest_comparison.loc[best_return_idx, 'total_return']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
